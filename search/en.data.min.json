[{"id":0,"href":"/efficient-web-app-architecture/REST/versioning/","title":"Versioning","parent":"REST","content":"It is important to make sure you don\u0026rsquo;t break the system for current consumers when you update your APIs. They need to be able to use it as usual while new consumers will use the new features. For this you need to version yout API. There are several ways to do so.\nURI versioning Each time you modify the web API or change the schema of resources, you add a version number to the URI for each resource. The previously existing URIs should continue to operate as before, returning resources that conform to their original schema.\nExtending the previous example, if the address field is restructured into subfields containing each constituent part of the address (such as streetAddress, city, state, and zipCode), this version of the resource could be exposed through a URI containing a version number, such as https://domain-name.com/v2/customers/3:\nThis versioning mechanism is very simple but depends on the server routing the request to the appropriate endpoint. it can become unwieldy as the web API matures through several iterations and the server has to support a number of different versions. Also, in all cases the client applications are fetching the same data (customer 3), so the URI should not really be different depending on the version. This scheme also complicates implementation of HATEOAS as all links will need to include the version number in their URIs.\nQuery string versioning Rather than providing multiple URIs, you can specify the version of the resource by using a parameter within the query string appended to the HTTP request, such as https://domain-name.com/customers/3?version=2. The version parameter should default to a meaningful value such as 1 if it is omitted by older client applications.\nThis approach has the semantic advantage that the same resource is always retrieved from the same URI, but it depends on the code that handles the request to parse the query string and send back the appropriate HTTP response. This approach also suffers from the same complications for implementing HATEOAS as the URI versioning mechanism.\nHeader versioning Rather than appending the version number as a query string parameter, you could implement a custom header that indicates the version of the resource. This approach requires that the client application adds the appropriate header to any requests, although the code handling the client request could use a default value (version 1) if the version header is omitted. The following examples use a custom header named Custom-Header. The value of this header indicates the version of web API.\nGET https://domain-name.com/customers/3 HTTP/1.1 Custom-Header: api-version=1\nGET https://domain-name.com/customers/3 HTTP/1.1 Custom-Header: api-version=2\nAs with the previous two approaches, implementing HATEOAS requires including the appropriate custom header in any links.\nMedia type versioning When a client application sends an HTTP GET request to a web server it should stipulate the format of the content that it can handle by using an Accept header, as described earlier in this guidance. Frequently the purpose of the Accept header is to allow the client application to specify whether the body of the response should be XML, JSON, or some other common format that the client can parse. However, it is possible to define custom media types that include information enabling the client application to indicate which version of a resource it is expecting.\nThe following example shows a request that specifies an Accept header with the value application/vnd.domain-name.v1+json. The vnd.domain-name.v1 element indicates to the web server that it should return version 1 of the resource, while the json element specifies that the format of the response body should be JSON:\nGET https://domain-name.com/customers/3 HTTP/1.1 Accept: application/vnd.adventure-works.v1+json\nThe code handling the request is responsible for processing the Accept header and honoring it as far as possible (the client application may specify multiple formats in the Accept header, in which case the web server can choose the most appropriate format for the response body). The web server confirms the format of the data in the response body by using the Content-Type\nHTTP/1.1 200 OK Content-Type: application/vnd.domain-name.v1+json; charset=utf-8 {\u0026#34;id\u0026#34;:3,\u0026#34;name\u0026#34;:\u0026#34;Contoso LLC\u0026#34;,\u0026#34;address\u0026#34;:\u0026#34;1 Microsoft Way Redmond WA 98053\u0026#34;} If the Accept header does not specify any known media types, the web server could generate an HTTP 406 (Not Acceptable) response message or return a message with a default media type.\nThis approach is arguably the purest of the versioning mechanisms and lends itself naturally to HATEOAS, which can include the MIME type of related data in resource links.\nNote When you select a versioning strategy, you should also consider the implications on performance, especially caching on the web server. The URI versioning and Query String versioning schemes are cache-friendly inasmuch as the same URI/query string combination refers to the same data each time.\nThe Header versioning and Media Type versioning mechanisms typically require additional logic to examine the values in the custom header or the Accept header. In a large-scale environment, many clients using different versions of a web API can result in a significant amount of duplicated data in a server-side cache. This issue can become acute if a client application communicates with a web server through a proxy that implements caching, and that only forwards a request to the web server if it does not currently hold a copy of the requested data in its cache.\n"},{"id":1,"href":"/efficient-web-app-architecture/REST/api/","title":"API","parent":"REST","content":" Overview A REST API (also known as RESTful API) is an Application Programming Interface that conforms to the constraints of REST architectural style and allows for interaction with RESTful web services. REST stands for REpresentational State Transfer and was created by Roy Fielding in 2000 who also co-founded the Apache HTTP Server project and has been heavily involved in the development of HTML.\nWhat is an API An API helps you communicate with a system so it can understand and fulfill the request.\nYou can think of an API as a mediator between the users or clients and the resources or web services they want to get. It’s also a way for an organization to share resources and information while maintaining security, control, and authentication—determining who gets access to what.\nOpen API Initiative The Open API Initiative was created to standardize REST API descriptions across vendors. As part of this initiative, the Swagger 2.0 specification was renamed the OpenAPI Specification (OAS) and brought under the Open API Initiative.\nThe OpenAPI Specification comes with a set of opinionated guidelines on how a REST API should be designed. That has advantages for interoperability, but requires more care when designing your API to conform to the specification.\nOpenAPI promotes a contract-first approach, rather than an implementation-first approach. Contract-first means you design the API contract (the interface) first and then write code that implements the contract.\nTools like Swagger can generate client libraries or documentation from API contracts.\n(read more about Open API specifications)[https://oai.github.io/Documentation/specification.html]\n"},{"id":2,"href":"/efficient-web-app-architecture/microservices/overview/","title":"Challenges","parent":"Microservices","content":"If your client is thinking about shifting to a microservices architecture, he also needs to change the way people work, not just the apps. Organizational and cultural changes are identified as challenges in part because each team will have its own deployment cadence and will be responsible for a unique service with its own set of customers. Those may not be typical developer concerns, but they will be essential to a successful microservices architecture.\nBeyond culture and process, complexity and efficiency are two major challenges of a microservice-based architecture.\nHow to Decompose One of the ways to make a good split between services could be to define services corresponding to business capabilities. A business capability is something a business does in order to provide value to its end users.\nIdentifying business capabilities and corresponding services requires a high level understanding of the business.\nOnce the business capabilities have been identified, the required services can be built corresponding to each of these identified business capabilities.\nEach service can be owned by a different team who becomes an expert in that particular domain and an expert in the technologies that are best suited for those particular services. This often leads to more stable API boundaries and more stable teams.\nBuilding and Deploying After deciding on the service boundaries of these small services, they can be developed by one or more small teams using the technologies which are best suited for each purpose. For example, you may choose to build a User Service in NodeJS with a MySQL database and a Product Recommendation Service with Java and MongoDB.\nOnce developed, CI/CD pipelines can be setup with any of the available CI servers (Jenkins, TeamCity, etc.) to run the automated test cases and deploy these service independently to different environments (Integration, QA, Staging, Production, etc).\nDesign the Individual Services Carefully When designing the services, carefully define them and think about what will be exposed, what protocols will be used to interact with the service, etc.\nIt is very important to hide any complexity and implementation details of the service and only expose what is needed by the service’s clients. If unnecessary details are exposed, it becomes very difficult to change the service later as there will be alot of work to determine who is relying on the various parts of the service. Additionally, a great deal of flexibility is lost in being able to deploying the service independently.\nDecentralize Things There are organizations who have found success with microservices and have followed a model where the teams who build the services take care of everything related to that service. They are the ones who develop, deploy, maintain and support it. There are no separate support or maintenance teams.\nAnother way to achieve the same is to have an internal open source model. By taking this approach, the developer who needs changes in a service can check out the code, work on a feature, and submit a PR himself instead of waiting for the service owner to pickup and work on needed changes.\nFor this model to work properly, the proper technical documentation is needed along with setup instructions and guidance for each service so that it’s easy for anyone to pickup and work on the service.\nAnother hidden advantage of this approach is that it keeps developers focused on writing high quality code as they know that others will be looking at it.\nThere are also some architectural patterns which can help in decentralizing things. For example, you might have an architecture where the collection of services are communicating via a central message bus. This bus handles the routing of messages from different services. Message brokers like RabbitMQ are a good example.\nWhat tends to happen over time is people start putting more and more logic into this central bus and it starts knowing more and more about your domain. As it becomes more intelligent, that should be avoided as it becomes difficult to make changes which require coordination across separate dedicated teams.\nA general advice for those types of architectures would be to keep them relatively “dumb” and let them just handle the routing. Event based architectures seem to work quite well in those scenarios.\nDeploy It is important to write Consumer Driven Contracts for any API that is being depended upon. This is to ensure that new changes in that API don’t break your API.\nIn Consumer Driven Contracts, each consumer API captures their expectations of the provider in a separate contract. All of these contracts are shared with the provider so that they gain insight into the obligations they must fulfill for each individual client.\nConsumer Driven Contracts must pass completely before being deployed and before any changes are made to the API. It also helps the provider to know what services are depending on it and how other services are depending on it.\nMaking Standards When there are multiple teams taking care of different services independently, it’s best to introduce some standards and best practices — error handling, for example. As might be expected, standards and best practices are not provided, each service would likely handle errors differently, and no doubt a significant amount of unnecessary code would be written several times.\nCreating standards is always helpful in long run. It is also important to let others know what an API does and documentation of the API should always be done when creating it. There are tools like Swagger which are very helpful in assisting in development across the entire API lifecycle, from design and documentation, to test and deployment. An ability to create metadata for your API and let users play with it, allows them to know more about it and use it more effectively.\nService Dependencies Over time, each service starts depending on more and more services. This can introduce more problems as the services grow, for example, the number of service instances and their locations (host+port) might change dynamically. Also, the protocols and the format in which data is shared might vary from service to service.\nHere’s where API Gateways and Service Discovery become very helpful. Implementing an API Gateway becomes a single entry point for all clients, and API Gateways can expose a different API for each client.\nThe API gateway might also implement security such as verifying that the client is authorized to perform the request. There are some tools which can be used for Service Discovery.\nFailure An important point to understand is that microservices are not resilient by default. There will be failures in services. Failures can happen because of failures in dependent services. Additionally, failures can arise for a variety of reasons such as bugs in code, network time outs, etc.\nWhat’s critical with a microservices architecture is to ensure that the whole system is not impacted or goes down when there are errors in an individual part of the system.\nThere are patterns like Bulkhead and Circuits Breaker which can help achieve better resiliency\nMonitoring and Logging Microservices are distributed by nature and monitoring and logging of individual services can be a challenge. It is difficult to go through and correlate logs of each service instance and figure out individual errors. Just as with monolithic applications, there is no single place to monitor microservices.\nTo solve such problems, a preferred approach is to take advantage of a centralized logging service that aggregate logs from each service instance. Users can search through these logs from one centralized spot and configure alerts when certain messages appear.\nStandard tools are available and widely used by various enterprises. They collects and aggregate logs which can be searched via a Kibana dashboard indexed by Elasticsearch.\n"},{"id":3,"href":"/efficient-web-app-architecture/microservices/mono-repo-vs-multi-repo/conclusion/","title":"Conclusion","parent":"Mono or multi repository","content":" How to choose? Different companies and projects will benefit from one strategy or the other based on their unique conditions. Here is a list of criteria you can use to help you chose :\nAre different programming languages involved? Do they require a particular software installed or special hardware to run? Mono if only one language. Multi if several How many deployment tools are required, and how complex are they to set up? Mono if only one tool. Multi if several What is the culture in the company? Are teams encouraged to collaborate or work independently ? How big is the codebase? Mono for smaller ones, multi for biggest ones How many people will work on the codebase? Mono for smaller ones, multi for biggest ones How many packages will there be? The answer for this question dependents of the other questions. How many packages does the team need to work on at a given time? Multi for smaller ones, mono for biggest ones How tightly coupled are the packages? Mono if tightly coupled, multi otherwise Summary Both mono-repo and multi-repo are equally popular and the choice depends on your project size, project requirements, and the level of versioning and access control you need.\nMono-repo favors consistency, whereas multi-repo focuses on decoupling.\nMulti-repo will favor faster development and delivery on one hand but will need more care on the CI/CD side. So ultimately you need to decide at which project/teams size adding those devops tasks is less expensive than the benefits of faster development.\nGiven our structure and the size of projects we usually develop, we favor multi-repo as it facilitates team autonomy and fast delivery.\n"},{"id":4,"href":"/efficient-web-app-architecture/REST/design/","title":"Design","parent":"REST","content":" Overview A well-designed web API should aim to support two key principles:\nPlatform independence. Any client should be able to call the API, regardless of how the API is implemented internally. This requires using standard protocols, and having a mechanism whereby the client and the web service can agree on the format of the data to exchange.\nService evolution. The web API should be able to evolve and add functionality independently from client applications. As the API evolves, existing client applications should continue to function without modification. All functionality should be documented so that client applications can fully use it.\nOrganize around resources REST APIs are designed around resources, which are any kind of object, data, or service that can be accessed by the client.\nA resource has an identifier, which is a URI that uniquely identifies that resource. For example, the URI for a particular customer order might be: https://domain-name.com/orders/1.\nFocus on the business entities that the web API exposes. For example, in an e-commerce system, the primary entities might be customers and orders. Creating an order can be achieved by sending an HTTP POST request that contains the order information. The HTTP response indicates whether the order was placed successfully or not.\nWhen possible, resource URIs should be based on nouns (the resource) and not verbs (the operations on the resource).\nhttps://domain-name.com/orders // Good https://domain-name.com/create-order // Avoid By constructing URIs correctly, it is possible to sort and prioritize them and thus improve the understanding of the system.\nThe API should not be designed around an internal representation of the data. For example a database schema, but rather business entities that it represents, so a request for customer information might include the customer name, address and contact number. this information could be stored in one or more SQL tables but our client does not need to know that. This allows us to modify the schema of our database independently of the client requesting the resource.\nMain design principles of RESTful APIs using HTTP Clients interact with a service by exchanging representations of resources. Many web APIs use JSON as the exchange format. For example, a GET request to the URI listed above might return this response body: { \u0026#34;orderId\u0026#34;:1, \u0026#34;orderValue\u0026#34;:99.90, \u0026#34;productId\u0026#34;:1, \u0026#34;quantity\u0026#34;:1 } REST APIs use a uniform interface, which helps to decouple the client and service implementations. For REST APIs built on HTTP, the uniform interface includes using standard HTTP verbs to perform operations on resources. The most common operations are GET, POST, PUT, PATCH, and DELETE.\nREST APIs use a stateless request model. HTTP requests should be independent and may occur in any order. The only place where information is stored is in the resources themselves. This constraint enables web services to be highly scalable, because any server can handle any request from any client.\nREST APIs are driven by hypermedia links that are contained in the representation. For example, the following shows a JSON representation of an order. It contains links to get or update the customer associated with the order.\n{ \u0026#34;orderID\u0026#34;:3, \u0026#34;productID\u0026#34;:2, \u0026#34;quantity\u0026#34;:4, \u0026#34;orderValue\u0026#34;:16.60, \u0026#34;links\u0026#34;: [ {\u0026#34;rel\u0026#34;:\u0026#34;product\u0026#34;,\u0026#34;href\u0026#34;:\u0026#34;https://domain-name/customers/3\u0026#34;, \u0026#34;action\u0026#34;:\u0026#34;GET\u0026#34; }, {\u0026#34;rel\u0026#34;:\u0026#34;product\u0026#34;,\u0026#34;href\u0026#34;:\u0026#34;https://domain-name/customers/3\u0026#34;, \u0026#34;action\u0026#34;:\u0026#34;PUT\u0026#34; } ] } When a client request is made via a RESTful API, it transfers a representation of the state of the resource to the requester or endpoint. This information, or representation, is delivered in one of several formats via HTTP: JSON, HTML, plain text\u0026hellip; JSON is the most popular file format to use because, despite its name, it is language-agnostic, as well as readable by both humans and machines.\nHeaders and parameters contain important identifier information as to the request\u0026rsquo;s metadata, authorization, uniform resource identifier (URI), caching, cookies, and more. There are request headers and response headers, each with their own HTTP connection information and status codes.\nREST is considered easier to use than a prescribed protocol like SOAP (Simple Object Access Protocol), which has specific requirements like XML messaging, and built-in security and transaction compliance that make it slower and heavier.\n"},{"id":5,"href":"/efficient-web-app-architecture/","title":"Efficient web app architecture","parent":"","content":"The success of a modern web application is always closely connected to its architecture. Keeping pace with changing requirements is a challenging task and a minor mistake can cost you the life of your product.\nDesigning a modern web application architecture requires a professional and qualified architect who can understand the limitations and challenges that come along with it. This website can help architects to decide which framework and what architecture can empower their project.\nGetting Started "},{"id":6,"href":"/efficient-web-app-architecture/microservices/","title":"Microservices","parent":"Efficient web app architecture","content":"Microservices architecture refers to an architectural style for developing applications where the application is developed as a collection of services that are :\nSmall. faster to develop and Easier to understand. Highly maintainable and testable. It is also more resilient as if one piece fails, the whole app does not necessarily go down, Loosely coupled, thus highly scalable Independently deployable. Shorter development cycles Independently developed. Owned by a small team. More developers can work on the same big application Organized around business capabilities. Each service can be developed using the best language and technology for its own purpose. It allows a large application to be separated into small independent services. Each one having its own realm of responsibility, built to accommodate an application feature and handle discrete tasks.\nIt also enables the rapid, frequent and reliable delivery of large, complex applications.\nMicroservices have become more viable thanks to advancements in containerization technologies, thus being very well suited for cloud-based applications. It also enables an organization to evolve its technology stack more easily.\nTo serve a single user request, a microservices-based application can call on many internal microservices to compose its response.\nMicroservices are used to speed up development. It is common to compare microservices versus service-oriented architecture. Both have the same objective of breaking up monolithic applications into smaller components, but with different approaches.\nNode.js with its very small footprint is very well suited for this type of architecture.\n"},{"id":7,"href":"/efficient-web-app-architecture/microservices/mono-repo-vs-multi-repo/","title":"Mono or multi repository","parent":"Microservices","content":"There are two main strategies for hosting and managing code through Git: mono-repo and multi-repo. Both approaches have their pros and cons.\nWe can use either approach for any codebase in any language. You can use any of these strategies for projects containing a handful of libraries to thousands of them. Even if it involves a few team members or hundreds, or you want to host private or open-source code.\n"},{"id":8,"href":"/efficient-web-app-architecture/REST/operations/","title":"Operations","parent":"REST","content":" Define operations in terms of HTTP methods The HTTP protocol defines a number of methods that assign semantic meaning to a request. The common HTTP methods used by most RESTful web APIs are:\nGET retrieves a representation of the resource at the specified URI. The body of the response message contains the details of the requested resource. POST creates a new resource at the specified URI. The body of the request message provides the details of the new resource. Note that POST can also be used to trigger operations that don\u0026rsquo;t actually create resources. PUT either creates or replaces the resource at the specified URI. The body of the request message specifies the resource to be created or updated. PATCH performs a partial update of a resource. The request body specifies the set of changes to apply to the resource. DELETE removes the resource at the specified URI. The effect of a specific request should depend on whether the resource is a collection or an individual item. The following table summarizes the common conventions adopted by most RESTful implementations using the e-commerce example.\nResource POST GET PUT DELETE /customers Create a new customer Retrieve all customers Bulk update of customers Remove all customers /customers/1 Error Retrieve the details for customer 1 Update the details of customer 1 if it exists Remove customer 1 /customers/1/orders Create a new order for customer 1 Retrieve all orders for customer 1 Bulk update of orders for customer 1 Remove all orders for customer 1 /orders/2 Error Retrieve the details for order 2 Update the details of order 2 Remove order 2 Another example (books):\nResource POST GET PUT DELETE /books Create a new books Retrieve all books Bulk update of books Remove all books /books/1 Error Retrieve the details for book 1 Update the details of book 1 if it exists Remove book 1 /books/1/comments Create a new comment for book 1 Retrieve all comments for book 1 Bulk update of comments for book 1 Remove all comments for book 1 "},{"id":9,"href":"/efficient-web-app-architecture/intro/","title":"Overview","parent":"Efficient web app architecture","content":"Web application architecture is a blueprint of simultaneous interactions between components, databases, middleware systems, user interfaces, and servers in an application. It can also be described as the layout that logically defines the connection between the server and client-side for a better web experience.\nMarket trends keep changing, user expectations keep evolving, and the growth of a business is never-ending. A web app needs an architecture to lay a strong foundation, and without it, your business app will be diving in the big ball of mud architecture anti-pattern.\nA well-thought web app architecture can handle the various loads and adapt to the changing business requirement skillfully to deliver a fast user experience that further improves the app performance. You can also take on several development tasks at the same time by dividing the structure into several small modules, eventually reducing the development time as well. Furthermore, it becomes easier to integrate new functionalities without affecting the overall structure.\nAll applications are made up of two primary components Client-side (front-end) where the code is written in HTML, CSS, JavaScript and stored within the browser. It’s where user interaction takes place. Server-side (back-end) responds to HTTP requests. The code is written in Java, PHP, Ruby, Python, Nodejs, etc. Database which sends the requested data to the server-side. Types of Web Application Architecture It is always a good practice to select the most appropriate architecture considering various factors in mind, such as app logic, features, functionalities, business requirements, etc. The right architecture defines the purpose of your product as a whole.\nWeb applications are broadly divided into four types :\nSingle Page Application Architecture SPA (Single Page Applications) has been introduced to overcome the traditional limitations to achieve smooth app performance, intuitive, and interactive user experience.\nInstead of loading a new page, SPAs load a single web page and reload the requested data on the same page with dynamically updated content. The rest of the web page remains untouched. They are developed on the client-side using JavaScript frameworks as the entire logic is always shifted to the frontend.\nMicroservice Architecture Microservice architecture has become the best alternative to Service-Oriented Architecture (SOA) and monolithic architecture. The services are loosely coupled to be developed, tested, maintained, and deployed independently. Such services can also communicate with other services through APIs to solve complex business problems.\nDeployment of web apps using monolithic architecture is a cumbersome task because of its tightly coupled components. Microservices has resolved this issue by separating the application into multiple individual service components. It further simplifies the connectivity between service components and eliminates the need for service orchestration.\nServerless Architecture It is an architecture where the whole execution of code is taken care of by cloud service providers– no need to deploy them manually on your server. Serverless architecture is a design pattern where applications are built and run without any manual intervention on the servers that are managed by third-party cloud service providers like Amazon and Microsoft.\nIt lets you focus more on the quality of the product and complexity to make them highly scalable and reliable. Broadly, it is categorized into two types – Backend-as-a-Service (BaaS) and Function-as-a-Service (FaaS).\nBaaS lets developers focus on the front-end development tasks, eliminating the operations performed on the back-end.\nFaaS is an event-driven model that allows developers to break the applications into small functions to focus on the code and event triggers. The rest will be handled by the FaaS service providers such as AWS Lambda.\nProgressive Web Applications Google introduced Progressive Web Apps (PWAs) in 2015 to develop apps that offer rich and native functionality with enhanced capabilities, reliability, and easy installation.\nPWAs are compatible apps with any browser and can run on any device. You can easily adjust an app’s function to a tablet and a desktop as well. These apps can easily be discovered and shared through URL instead of the app store. Installation of these apps is also effortless and can be quickly added to a device’s home screen. These apps work efficiently on poor internet connectivity and in offline mode as well.\nWeb Application Architecture Best Practices \u0026amp; Tools Designing an architecture is just the first step, but the success of your web application depends a lot on the architectural patterns you choose. Replicating strategies of popular web apps can do more damage than good. To avoid such issues, you need to understand every aspect of the application and confront them to properly selected criterias that match the projects constraints such as :\nSystem flexibility and efficiency Component reusability Adapted structure of code High Scalability Stability and reliability "},{"id":10,"href":"/efficient-web-app-architecture/REST/","title":"REST","parent":"Efficient web app architecture","content":" What is REST REST is a set of architectural constraints. It is not a protocol nor a standard, so API developers can implement REST in a variety of ways. It is independent of any underlying protocol and is not necessarily tied to HTTP. However, most common REST API implementations use HTTP as the application protocol.\n"},{"id":11,"href":"/efficient-web-app-architecture/sqlnosql/","title":"SQL or NoSQL","parent":"Efficient web app architecture","content":"When developing a microservice API, in most cases you will need to store data in a database. The great thing about microservice architecture is that you have one database per microservice, meaning you can pick the best system for each one of them.\nDifferences between SQL \u0026amp; NoSQL SQL databases are relational, NoSQL databases are non-relational. SQL databases use structured query language and have a predefined schema. NoSQL databases have dynamic schemas for unstructured data. SQL databases are vertically scalable, while NoSQL databases are horizontally scalable. SQL databases are table-based, while NoSQL databases are document, key-value, graph, or wide-column stores. SQL databases use multi-row transactions, while NoSQL use unstructured data like documents or JSON. Structure SQL database schema always represent relational tabular data, with rules about consistency and integrity. They contain tables with columns (attributes) and rows (records), and keys have constrained logical relationships. This structure is the easiest to fathom because it is the one most people learnt and are used to work with.\nnoSQL strutures generally fit into one of those categories :\nColumn-oriented databases transpose row-oriented RDBMSs, allowing efficient storage of high-dimensional data and individual records with varying attributes. Key-Value stores are dictionaries which access diverse objects with a key unique to each. Document stores hold semi-structured data: objects which contain all of their own relevant information, and which can be completely different from each other. Graph databases add the concept of relationships (direct links between objects) to documents, allowing rapid traversal of greatly connected data sets. General rules NoSQL trades consistency for performance and scalability.\nWorking with these new structures will entirely change the way of developing your microservice in order to be really efficient.\nWhen to use Generally, NoSQL is preferred for:\nGraph or hierarchical data Data sets which are both large and mutate significantly, Businesses growing extremely fast but lacking data schemata. Big data RDBMs is more appropriate when :\ndata is conceptually modeled as tabular consistency is critical More rules Structure: Do not consider NoSQL if unstructured data does not work for your microservice (ie: you need consistency, integrity, no redundancy\u0026hellip;). Size: NoSQL is usually better for big data. Respone Time \u0026amp; access frequency: With the right architecture given the context, each solution can be very efficient. It is difficult to extract a general rule on the performance side. Except for very big datasets where NoSQL starts to really shine. Scaling: Horizontal scaling is usually easier with NoSQL. Performances NoSQL databases are specifically designed for unstructured data. A particular data entity is stored together and not partitioned. So performing read or write operations on a single data entity is very fast.\nSQL databases are normalized. The data is broken down into various logical tables. So SQL databases are fast for joins, queries, updates, etc.\nperfomance will depend on the structure of the data. So it all boils down to the need to get structured or unstructured data.\nDevelopment Speed With SQL, before entering data into the database, you need to define your schema with a list of columns, types, foreign keys etc. This is time-consuming during the conception phase.\nIn addition, each change in the table, like adding a column or changing an existing column, requires time to alter the schema before entering different data. Then you will need to update legacy data to fit the new structure.\nOn the other hand, with NoSQL databases, you don’t need to define a schema to start storing and retrieving data. if the data changes due to business requirements, you can just go ahead and store the data in the new format without restructuring your schema. This is a major advantage of NoSQL databases.\nHowever, because the data is unstructured and mostly unverified before it enters the database, different structures of the same data can exist. Malformed or incorrect data can be inserted and saved. Meaning reliability and consistency features will have to be architected and developed by the team, which adds more complexity to the system. The controllers will also need extra care to be able to handle different cases of data to correctly feed the views.\nRead / Write In SQL databases, the inserted data is validated to correspond to the schema of the table. This process takes time as we validate each data item against the corresponding column. Once we go schemaless, we can save this precious time. Thus, NoSQL databases provide a larger count of write operations per second as compared to SQL databases. This is especially useful for logging services that need to store huge amounts of log data.\nSQL databases, on the other hand, excel at efficiently reading large volumes of data from the database. In scenarios where you are doing multiple read operations per second, a traditional SQL database is very good.\nScaling SQL databases are vertically scalable. You are able to increase the load on a single server by adding more CPU, RAM, or SSD capacity. While horizontal scaling is possible with some SQL databases, NoSQL databases are a lot easier to scale horizontally. Making them ideal to handle higher traffic by adding more containers. Horizontal scaling has a greater overall capacity than vertical scaling, making NoSQL databases the preferred choice for large data sets.\nNote that each databases handle scaling differently. Some NoSQL databases can be impossible to scale horizontaly.\nInto the cloud Modern brands emphasize interactivity between end-users, justifying decentralized, cloud-based architectures, and exposing diverse new data needing representation. Enter NoSQL, champion of massive, distributed, and morphing data.\nBut if this non-relational interest caused traditional RDBMSs to flag at all, they are now resurging. SQL remains more accessible, understandable, and most-importantly \u0026ldquo;a good enough\u0026rdquo; solution in most cases. So why bother.\nNoSQL increasingly represents a set of technologies with generalist applicability and inclusiveness. Traditional SQL solutions are also rebranding as generalized databases and connecting with NoSQL. Clearly both paradigms remain equally valid in the modern transition to the cloud.\nConclusion SQL is old and sometimes constraining, but also time-tested and still considered the universal interface.\nNoSQL databases are new and flexible, but lack maturity. They require user specialization and a new way of thinking.\nBoth models are useful and growing together.\nUltimately, a technology is only valuable when it serves your business and increases ROI.\nAs usual, do not over-achitecture your project. An objective analysis with weighted criterias will help to make the good decision. SQL is still relevant in most situations and save a lot of precious time.\n"},{"id":12,"href":"/efficient-web-app-architecture/tags/","title":"Tags","parent":"Efficient web app architecture","content":""},{"id":13,"href":"/efficient-web-app-architecture/microservices/mono-repo-vs-multi-repo/mono-repo/","title":"What is Mono-repository","parent":"Mono or multi repository","content":"The mono-repo approach uses a single repository to host all the code for the multiple services composing a micro-services. At its most extreme, the whole codebase from a company — spanning various projects and coded in different languages — can be hosted in a single repository but here we will only talk about one micro-services application.\nAdvantages of Mono-repo A single place to store all the project code, and can be accessed by everyone in the team Easy to reuse and share code, collaborate with the team Easy to understand the impact of your change on the entire project Best option for code-refactoring and large changes to code Team members can get an overall view of the entire project Easy to manage dependencies Lighter workload on CI/CD and tests since their are global to the entire application. Lowers barriers of entry When new staff members start working for a company, they need to download the code and install the required tools to begin working on their tasks. Suppose the project is scattered across many repositories, each having its installation instructions and tooling required. Even if there are tools to automate this, it is still more compicated to understand and use. Plus someone had to write these automation at some point and still has to maintain it. There is a great chance that the documentation will not be complete anymore, requiring these new team members to reach out to colleagues for help.\nA monorepo simplifies matters. Since there is a single location containing all code and documentation, you can streamline the initial setup.\nCentrally located code management Having a single repository gives visibility of all the code to all developers. It simplifies code management since we can use a single issue tracker to watch all issues throughout the application’s life cycle.\nFor instance, these characteristics are valuable when an issue spans two (or more) child libraries with the bug existing on the dependent library. With multiple repositories, it may be challenging to find the piece of code where the problem happens.\nOn top of this, we would need to figure out which repository to use to create the issue and then invite and cross-tag members of other teams to help resolve the problem.\nWith a monorepo, both locating code problems and collaborating to troubleshoot become simpler to achieve.\nEasier application-wide refactoring When creating an application-wide refactoring of the code, multiple libraries will be affected. If you’re hosting them via multiple repositories, managing all the different pull requests to keep them synchronized with each other can prove to be a challenge.\nA monorepo makes it easy to perform all modifications to all code for all libraries and submit it under a single pull request.\nUniform development culture Since they will share the same repository, Teams will most likely share the same programming and management methodologies and use the same development tools.\nIssues of mono-repo Performance. If your project grows and more files are added everyday, the check-out, pull and other operations might become slow and file searches may take longer.\nIf you hire a lot of independent contractors for your project, giving them access to the entire code base may not be so secure.\nLarge companies that use mono-repos had to create customized tools to handle the scaling-up issues.\nSlower development cycles When the code for a library contains breaking changes, which make the tests for dependent libraries fail, the code must also be fixed before merging the changes.\nIf these libraries depend on other teams and they are busy working on some other task the development of the new feature may stall.\nThe project may well start advancing at the speed of the slowest team in the company. This outcome could frustrate the members of the fastest teams.\nThis is not the case for a multi-repo application where each service is independant and will have several versions to work with other services wether they are up to date or not.\nIn addition, a library will need to run the tests for all other libraries too. The more tests to run, the more time it takes to run them, slowing down how fast we can iterate on our code.\nRequires download of entire codebase When the mono-repo contains all the code for the application, it can be huge, containing gigabytes of data. To contribute to any library hosted within, anybody would require a download of the whole repository.\nDealing with a vast codebase implies a poor use of space on our hard drives and slower interactions with it. For instance, everyday actions such as executing git status or searching in the codebase with a regex may take many seconds or even minutes longer than they would with multiple repos.\nUnmodified libraries may be newly versioned When we tag the mono-repo, all code within is assigned the new tag. If this action triggers a new release, then all libraries hosted in the repository will be newly released with the version number from the tag, even though many of those libraries may not have had any change.\nForking is more difficult Open source projects must make it as easy as possible for contributors to become involved. With multiple repositories, contributors can head directly to the specific repository for the project they want to contribute to. With a mono-repo hosting various projects, though, contributors must first navigate their way into the right service and will need to understand how their contribution may affect all other services.\n"},{"id":14,"href":"/efficient-web-app-architecture/microservices/mono-repo-vs-multi-repo/multi-repo/","title":"What is multi-repository","parent":"Mono or multi repository","content":"The multi-repo approach uses several repositories to host the multiple libraries or services of a project. At its most extreme, it will host every minimum set of reusable code or standalone functionality (such as a microservice) under its own repository.\nAdvantages of Multi-repo Each service and library have its own versioning Code check-outs and pulls are small and separate, thus there are no performance issues even if the project size grows Teams can work independently and do not need access to the entire codebase Faster development and flexibility Each service can be released separately and have its own deployment cycle Better access control – all teams need not have full access to all the libraries – but can get read access if they need Independent library versioning When tagging a repository, its whole codebase is assigned the “new” tag. Since only the code for a specific library is on the repository, the library can be tagged and versioned independently of all other libraries hosted elsewhere.\nHaving an independent version for every library helps define the dependency tree for the application, allowing us to configure what version of each library to use.\nIndependent service releases Since the repository only contains the code for some service and nothing else, it can have its own deployment cycle, independently of any progress made on the applications accessing it.\nThe service can use a fast release cycle such as continuous delivery (where new code is deployed after it passes all the tests). Some libraries accessing the service may use a slower release cycle, such as those that only produce a new release once a week.\nHelps define access control across the organization Only the team members involved with developing a library need to be added to the corresponding repository and download its code. As a result, there’s an implicit access control strategy for each layer in the application. Those involved with the library will be granted editing rights, and everyone else may get no access to the repository. Or they may be given reading but not editing rights.\nAllows teams to work autonomously Team members can design the library’s architecture and implement its code working in isolation from all other teams. They can make decisions based on what the library does in the general context without being affected by the specific requirements from some external team or application.\nDisadvantages of multi-repo Dependencies and libraries used across services and projects have to be regularly synced to get the latest version Encourages a siloed culture, leading to duplicate code and individual teams trying to resolve the same problem Each team may follow a different set of best practices for their code causing difficulties in following common best practices Heavier workload on CI/CD and tests since their are separated or each service. Libraries must constantly be resynced When a new version of a library containing breaking changes is released, libraries depending on this library will need to be adapted to start using the latest version. If the release cycle of the library is faster than that of its dependent libraries, they could quickly become out of sync with each other.\nTeams will need to constantly catch up to use the latest releases from other teams. Given that different teams have different priorities, this may sometimes prove arduous to achieve.\nConsequently, a team not able to catch up may end up sticking to the outdated version of the depended-upon library. This outcome will have implications on the application (in terms of security, speed, and other considerations), and the gap in development across libraries may only get wider.\nMay fragment teams When different teams don’t need to interact, they may work in their own silos. In the long term, this could result in teams producing their subcultures within the company, such as employing different methodologies of programming or management or using different sets of development tools.\nIf some team member eventually needs to work in a different team, they may suffer a bit of culture shock and learn a new way of doing their job.\nMore devops tasks Some members of the team will need to maintain a more complex CI/CD pipeline and tests. In order to properly handle the benefits of a multi-repo approcha such as faster development and delivery.\n"},{"id":15,"href":"/efficient-web-app-architecture/mvc/","title":"What is the MVC pattern","parent":"Efficient web app architecture","content":"MVC is a paradigm concerned with how object oriented systems could have UIs. It helps break up the code into separate components. This way, it is much easier to manage and make changes to either side without them interfering with each other.\nIt is a simple concept. The code is divided into three components :\nModel for data processing and business logic View for the graphical user interface representing the model state Controller for the link between View and models. The brains that controls how data is displayed The controller takes in inputs from the route and turns them into commands which the model can interpret. then it receives the end result from the model and turns it into a response the view can display.\nIn a web application you can summarize the commmunication between front-end and back-end as a client sending REST request to a server, the routing matching the requested URL to the controller action, which calls the model(s) for data gathering \u0026amp;processing and return the result back to the client as a HTML page (view) or Json data (API).\nEarly web frameworks took the general idea of separating out business, data and view logic and applied the principle to how they structured the web application. For example Symfony follows MVC guidelines and Angular MVVM is derived from MVC as well.\nMVC applied to REST APIs For a modern web application using REST and a static website as front-end, the REST API does not generate a GUI anymore. Most of the time it returns data in JSON format.\nThe generated JSON can be thought of as the \u0026lsquo;view\u0026rsquo; but other than this the \u0026lsquo;View\u0026rsquo; part of the MVC pattern is not relevant here. Most importantly it is not telling us how to properly separate our code.\nMRC pattern We will replace \u0026lsquo;views\u0026rsquo; by \u0026lsquo;routes\u0026rsquo; in order to have a clear list of routes with no business logic in it.\nSo we replace \u0026lsquo;MVC pattern\u0026rsquo; by \u0026lsquo;MRC pattern\u0026rsquo; and separate our code into 3 folders :\nModels/ for handling all of the data and business logic. Routes/ for a clear list of routes available Controllers/ for the link between routes and models. The models folder may have separate sub folders (services, entities) depending on the database and the ORM you will use. In any case you need to understand that we are talking about models in an MVC point of view here. Meaning that it is not just the data model. It includes all the business logic (services, data models/entities\u0026hellip;)\nIn a micro service architecture \u0026lsquo;routes\u0026rsquo; and \u0026lsquo;controllers\u0026rsquo; folders should not be too crowded, So no subfolders should be needed. If you feel that you need one then maybe it is time think of creating another micro service.\nYou will learn more on how to separate your code in the express chapter.\n"}]